# -*- coding: utf-8 -*-
"""ShownTellMain

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17tOSaRZBXiRsdYC0vun-Hg_gk9yz75Bz
"""

# Perform standard imports
import spacy
nlp = spacy.load('en_core_web_sm')
from nltk import ngrams
from collections import Counter
import re
import numpy as np
from fastapi import FastAPI,Path
# import nest_asyncio
# from pyngrok import ngrok
import uvicorn
from pydantic import BaseModel
from typing import Optional
from fastapi import APIRouter
from spacy.matcher import Matcher
import pandas as pd

matcher = Matcher(nlp.vocab)
router = APIRouter()

class Essay(BaseModel):
  EssayText:str

@router.post("/showntell_essay/")
async def showntell_essay(InputEssay:Essay):
  nlp = spacy.load('en_core_web_sm')
  doc  = nlp(InputEssay.EssayText)
  mental_tells = ['loved','realized','thought','hoped','considered','wondered','prayed','knew','saw','watched','heard','felt','could','see','seemed','appeared',
  'looked','believed','reflected','disgusted','feared','show','noticed','smelled','wonder','walked','come','hate','decided','wished','feel','see','smell',
  'fell']

  emotional_tells = ['adoration','agitation','amazement','amusement','anger','anguish','annoyance','anticipation','anxiety','confidence','conflicted','confusion',
  'contempt','curiosity','defeat','defensiveness','denial','depression','desire','desperation','determination','disappointment','disbelief','disgust','doubt','dread',
  'eagerness','elation','embarrassment','envy','excitement','fear','frustration','gratitude','guilt','happiness','hatred','hopefulness','humiliation','hurt','impatience',
  'indifference','insecurity','irritation','jealousy','loneliness','love','nervousness','nostalgia','overwhelmed','paranoia','peacefulness','pride','rage','regret','relief',
  'reluctance','remorse','resentment','resignation','sadness','satisfaction','scorn','shame','skepticism','smugness','somberness','surprise','shock','suspicion','sympathy',
  'terror','uncertainty','unease','wariness','worry']

  motivational_tells = ['decided','because','tried','when']

  emotional_adjectives = ['frustated', 'happy', 'tall', 'angry', 'sad', 'hungry', 'excited', 'embarrased', 'bright', 'shocked', 'hot', 'beautiful', 
                          'afraid', 'cold', 'interesting', 'confused', 'sweet', 'different', 'scared', 'mournful', 'furious', 'overwhelmed', 'stressed', 
                          'unique', 'overjoyed', 'scarier', 'tired', 'shy', 'giddy', 'anxious','chilly','friendly','ghastly','ghostly','holy','kingly',
                          'knightly','lonely','lovely','orderly','prickly','queenly','surly','ugly','worldly','wrinkly']

  adverbs_avoid = ['very', 'really', 'spectacularly','already' 'abruptly', 'absently', 'absentmindedly', 'accusingly', 'actually', 'adversely', 'affectionately', 
                  'amazingly', 'angrily', 'anxiously', 'arrogantly', 'bashfully', 'beautifully', 'boldly', 'bravely', 'breathlessly', 'brightly', 'briskly', 'broadly', 
                  'calmly', 'carefully', 'carelessly', 'certainly', 'cheaply', 'cheerfully', 'cleanly', 'clearly', 'cleverly', 'closely', 'clumsily', 'coaxingly', 'commonly', 
                  'compassionately', 'conspicuously', 'continually', 'coolly', 'correctly', 'crisply', 'crossly', 'curiously', 'daintily', 'dangerously', 'darkly', 'dearly', 
                  'deceivingly', 'delicately', 'delightfully', 'desperately', 'determinedly', 'diligently', 'disgustingly', 'distinctly', 'doggedly', 'dreamily', 'emptily', 
                  'energetically', 'enormously', 'enticingly', 'entirely', 'enviously', 'especially', 'evenly', 'exactly', 'excitedly', 'exclusively', 'expertly', 'extremely', 
                  'fairly', 'faithfully', 'famously', 'fearlessly', 'ferociously', 'fervently', 'finally', 'foolishly', 'fortunately', 'frankly', 'frantically', 'freely', 
                  'frenetically', 'frightfully', 'fully', 'furiously', 'generally', 'generously', 'gently', 'gleefully', 'gratefully', 'greatly', 'greedily', 'grumpily', 
                  'guiltily', 'happily', 'harshly', 'hatefully', 'heartily', 'heavily', 'helpfully', 'helplessly', 'highly', 'hopelessly', 'hungrily', 'immediately', 'importantly', 
                  'impulsively', 'inadvertently', 'increasingly', 'incredibly', 'innocently', 'instantly', 'intensely', 'intently', 'inwardly', 'jokingly', 'kindly', 'knowingly', 
                  'lawfully', 'lightly', 'likely', 'longingly', 'loudly', 'madly', 'marvelously', 'meaningfully', 'mechanically', 'meekly', 'mentally', 'messily', 'mindfully', 'miserably', 
                  'mockingly', 'mostly', 'mysteriously', 'naturally', 'nearly', 'neatly', 'negatively', 'nervously', 'nicely', 'obviously', 'occasionally', 'oddly', 'openly', 'outwardly', 
                  'partially', 'passionately', 'patiently', 'perfectly', 'perpetually', 'playfully', 'pleasantly', 'pleasingly', 'politely', 'poorly', 'positively', 'potentially', 'powerfully', 
                  'professionally', 'properly', 'proudly', 'quaveringly', 'queerly', 'quickly', 'quietly', 'quintessentially', 'rapidly', 'rapturously', 'ravenously', 'readily', 'reassuringly', 
                  'regretfully', 'reluctantly', 'reproachfully', 'restfully', 'righteously', 'rightfully', 'rigidly', 'rudely', 'sadly', 'safely', 'scarcely', 'searchingly', 'sedately', 
                  'seemingly', 'selfishly', 'separately', 'seriously', 'sharply', 'sheepishly', 'sleepily', 'slowly', 'slyly', 'softly', 'solidly', 'speedily', 'sternly', 'stingily', 'strictly', 
                  'stubbornly', 'successfully', 'superstitiously', 'surprisingly', 'suspiciously', 'sympathetically', 'tenderly', 'terribly', 'thankfully', 'thoroughly', 'thoughtfully', 'tightly', 
                  'totally', 'tremendously', 'triumphantly', 'truly', 'truthfully', 'understandably', 'unfairly', 'unfortunately', 'unhappily', 'unwillingly', 'urgently', 'usually', 'utterly', 'vastly', 
                  'venomously', 'viciously', 'violently', 'warmly', 'wearily wholly', 'wildly', 'wilfully', 'wisely', 'wonderfully', 'wonderingly', 'worriedly']

  aux_tell_list = []
  ment_tell_list = []
  det_tell_list = []
  motiv_tell_list = []
  emot_adj_tell_list  = []
  adv_tell_list  = []
  adj_tell_list = []
  sent_level_dict = {}
  count = 0

  output_dict = {}

  adj_dict_inner = {}
  adv_dict_inner = {}
  aux_dict_inner = {}
  emot_dict_inner = {}
  det_dict_inner = {}
  ment_dict_inner = {}
  motiv_dict_inner = {}

  doc_sents = [sent for sent in doc.sents]
  len_tot_sents = len(doc_sents)
  for sents in doc.sents:
    count = 0
    for ix,token in enumerate(sents):
      tok_pos=token.idx
      if token.is_sent_start:    # checking for first token
        if sents[ix].tag_ == 'PRP' or sents[ix].tag_ == 'PRP$' or sents[ix].tag_ == 'NN' :        # check for 'PRP' (Pronoun Personal) specifically with  'I','We','They','He','She'. 'You'.,check for 'PRP$' (Pronoun Possessive) my, our, your, his, her, its, and their,check for 'NN' (noun, singular or mass) i.e. 'Non-specific Nouns'
          if sents[ix + 1].pos_ == 'AUX' or sents[ix + 1].pos_ == 'MD':                           # check for the token to the right for 'AUX' and 'MD'.
            tok = str(sents[ix:ix+2])     # getting the right token as well
            right_pos = sents[ix+2].idx # getting the right 2 token's starting character offset.                                                    
            aux_dict_inner["startIndex"] = tok_pos
            aux_dict_inner["endIndex"]   = right_pos
            aux_dict_inner["text"]       = tok
            aux_tell_list.append(aux_dict_inner.copy())          
            count +=1
            sent_level_dict[sents] = count
          elif sents[ix + 1].pos_ == 'VERB':
            if sents[ix+1].text.lower() in mental_tells or sents[ix+1].lemma_.lower() in mental_tells: # check if the token next to the first token is a 'VERB' out of 'mental tell' verbs.
              tok = str(sents[ix:ix+2])                                                                  # check for 'NN' (noun, singular or mass) i.e. 'Non-specific Nouns'
              right_pos = sents[ix+2].idx # getting the right 2 token's starting character offset.                                                    
              ment_dict_inner["startIndex"] = tok_pos
              ment_dict_inner["endIndex"]   = right_pos
              ment_dict_inner["text"]       = tok
              ment_tell_list.append(ment_dict_inner.copy())                
              count +=1
              sent_level_dict[sents] = count            
        elif sents[ix].pos_ == 'DET':                                                             # check if the first token is a 'determiner' 
          if sents[ix + 1].pos_ == 'ADJ' or sents[ix + 1].tag_ == 'RBS':                          # and next one is 'adverb superlative' or an 'adjective'.
              tok = str(sents[ix:ix+2])
              right_pos = sents[ix+2].idx # getting the right 2 token's starting character offset.                                                    
              det_dict_inner["startIndex"] = tok_pos
              det_dict_inner["endIndex"]   = right_pos
              det_dict_inner["text"]       = tok              
              det_tell_list.append(det_dict_inner.copy())
              count +=1
              sent_level_dict[sents] = count            
      if token.text.lower() in motivational_tells or token.lemma_.lower() in motivational_tells:   # check if the token is out of the 'motivational tell' list
          tok = token.text.lower()
          right_pos = sents[ix+1].idx # getting the right 2 token's starting character offset.                                                    
          motiv_dict_inner["startIndex"] = tok_pos
          motiv_dict_inner["endIndex"]   = right_pos
          motiv_dict_inner["text"]       = tok       
          motiv_tell_list.append(motiv_dict_inner.copy())
          count +=1
          sent_level_dict[sents] = count        
      elif sents[ix].text.lower() == 'to' and sents[ix + 1].pos_ == 'VERB':                        # check if the word is of the form 'to [Verb]'
          tok = str(sents[ix:ix+2])
          right_pos = sents[ix+2].idx # getting the right 2 token's starting character offset.                                                    
          motiv_dict_inner["startIndex"] = tok_pos
          motiv_dict_inner["endIndex"]   = right_pos
          motiv_dict_inner["text"]       = tok       
          motiv_tell_list.append(motiv_dict_inner.copy())
          count +=1
          sent_level_dict[sents] = count        
      elif any(x in sents[ix].text.lower() for x in ['with','in']) and (sents[ix + 1].pos_ == 'NOUN' or sents[ix + 1].pos_ == 'PROPN') and sents[ix + 1].text.lower() in emotional_tells: # check if the word is of the form 'with [noun] or in [noun]'
          tok = str(sents[ix:ix+2])
          right_pos = sents[ix+2].idx # getting the right 2 token's starting character offset.  
          emot_dict_inner["startIndex"] = tok_pos
          emot_dict_inner["endIndex"]   = right_pos
          emot_dict_inner["text"]       = tok                                                         
          emot_adj_tell_list.append(emot_dict_inner.copy())
          count +=1
          sent_level_dict[sents] = count        
      elif token.pos_ == 'ADJ' and (token.text.lower() in emotional_adjectives or token.lemma_.lower() in emotional_adjectives):  # check if the word is out of the list of 'emotional' adjective word list
        tok = token.text.lower()
        right_pos = sents[ix+1].idx # getting the right 2 token's starting character offset.                                                    
        adj_dict_inner["startIndex"] = tok_pos
        adj_dict_inner["endIndex"]   = right_pos
        adj_dict_inner["text"]       = tok     
        adj_tell_list.append(adj_dict_inner.copy())
        count +=1
        sent_level_dict[sents] = count      
      elif token.pos_ == 'ADV' and (token.text.lower() in adverbs_avoid or token.lemma_.lower() in adverbs_avoid):             # check if the word is out of the list of 'adverb to avoid' word list
        tok_pos=token.idx
        right_pos = sents[ix+1].idx # getting the right token's starting character offset.
        tok = token.text.lower()
        adv_dict_inner["startIndex"] = tok_pos
        adv_dict_inner["endIndex"]   = right_pos
        adv_dict_inner["text"]       = tok  
        adv_tell_list.append(adv_dict_inner.copy())
        count +=1
        sent_level_dict[sents] = count   

  output_dict['TELL_REASON_ADJ']    = dict(color = "#FFA500", description = "Adjective tell", highlights= adj_tell_list)
  output_dict['TELL_REASON_ADV']    = dict(color = '#90EE90', description = "Adverbial tell", highlights = adv_tell_list)
  output_dict['TELL_REASON_AUX']    = dict(color = '#FFA07A', description = "Auxilary tell", highlights = aux_tell_list)
  output_dict['TELL_REASON_DET']    = dict(color = '#778899', description = "Determiner tell",highlights = det_tell_list)
  output_dict['TELL_REASON_EMOT']   = dict(color = '#B0C4DE', description = "Emotional tell", highlights = emot_adj_tell_list)
  output_dict['TELL_REASON_MENTAL'] = dict(color = '#3CB371', description = "Mental tell", highlights = ment_tell_list)
  output_dict['TELL_REASON_MOTIV']  = dict(color = '#7B68EE', description = "Motivational tell", highlights = motiv_tell_list)

  return (output_dict)

@router.post("/checkForPassiveType/")
async def check_passive_voice(inputEssay:Essay):
  passive_dict = {}
  # # running the model on sentence
  doc = nlp(inputEssay.EssayText)
  dep_list = []
  passive_list = []
  passive_dict_inner = {}

  passive_rule2 = [{'DEP': 'auxpass'}]
  matcher.add('Passive2',[passive_rule2])

  for sents in doc.sents:
    dep_list = []
    offset_list = []
    for token in sents:# Tokenize the sentence into words/tokens
      #print(token.text, token.pos_, token.dep_)  # form a list of various syntactic dependencies  
      dep_list.append(token.dep_)

    if ('nsubjpass') in dep_list or ('auxpass') in dep_list:
      passive_dict_inner["startIndex"] = sents.start_char
      passive_dict_inner["endIndex"]   = sents.end_char
      passive_dict_inner["text"]       = sents.text 

      # # running the model on sentence
      doc_sent = nlp(sents.text)
      #print(doc_sent)
      found_matches = matcher(doc_sent)
      #print(found_matches)
      for match_id, start, end in found_matches:
        string_id = nlp.vocab.strings[match_id]  # get string representation
        span = doc_sent[start:end+1]                    # get the matched span
        #print(span)
        offset_list.extend((match.start(),match.end()+1) for match in re.finditer(str(span), doc.text.lower())) # need to be done at whole document level and not sentence level to get offset from 1.
      
      passive_dict_inner["characteroffset"] = offset_list
      passive_list.append(passive_dict_inner.copy())      
      #passive_list.append(dict(text = sents,startIndex = sents.start_char,endIndex = sents.end_char))

  passive_dict = dict(color = "#FFA500", description = "Passive voice",passive_voice = passive_list)
  return (passive_dict)

# Sentence type functionality start
SUBJECTS = ["nsubj", "nsubjpass", "csubj", "csubjpass", "agent", "expl"]
OBJECTS = ["dobj", "dative", "attr", "oprd"]

def getSubsFromConjunctions(subs):
    moreSubs = []
    for sub in subs:
        # rights is a generator
        rights = list(sub.rights)
        #print(rights)
        rightDeps = {tok.lower_ for tok in rights}
        if "and" in rightDeps:
            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == "NOUN"])
            if len(moreSubs) > 0:
                moreSubs.extend(getSubsFromConjunctions(moreSubs))
    return moreSubs

def findSubs(tok):
    head = tok.head
    while head.pos_ != "VERB" and head.pos_ != "NOUN" and head.head != head:
        head = head.head
    if head.pos_ == "VERB":
        subs = [tok for tok in head.lefts if tok.dep_ == "SUB"]
        if len(subs) > 0:
            verbNegated = isNegated(head)
            subs.extend(getSubsFromConjunctions(subs))
            return subs, verbNegated
        elif head.head != head:
            return findSubs(head)
    elif head.pos_ == "NOUN":
        return [head], isNegated(tok)
    return [], False

def isNegated(tok):
    negations = {"no", "not", "n't", "never", "none"}
    for dep in list(tok.lefts) + list(tok.rights):
        if dep.lower_ in negations:
            return True
    return False

def findSVs(tokens):
    svs = []
    verbs = [tok for tok in tokens if tok.pos_ == "VERB" or tok.pos_ == "AUX"]
    for v in verbs:
        subs, verbNegated = getAllSubs(v)
        if len(subs) > 0:
          svs.append((subs[-1].orth_, "!" + v.orth_ if verbNegated else v.orth_))
            # for sub in subs:
            #     svs.append((sub.orth_, "!" + v.orth_ if verbNegated else v.orth_))
    return svs

def getAllSubs(v):
    verbNegated = isNegated(v)
    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != "DET"]
    #print(subs)
    if len(subs) > 0:
        subs.extend(getSubsFromConjunctions(subs))
    else:
        foundSubs, verbNegated = findSubs(v)
        subs.extend(foundSubs)
    return subs, verbNegated

def testSVOs(input_sent):
    tok = nlp(input_sent)
    svs = findSVs(tok)
    return (len(svs))

@router.post("/checkForSentType/")
async def check_sentence_type(inputEssay:Essay):
  s_conj = ['after','although','as','because','before','even though','if','since','though','unless','until','when','whenever','whereas','wherever','while','where','that','who','no matter']
  exceptions = ['as soon as']

  complex_comp_list=[]
  complex_list=[]
  compound_list=[]
  simple_list=[]
  dep_list = []
  ret_dict = {}
  docu = nlp(inputEssay.EssayText)
  simple_dict_inner = {}
  complex_comp_dict_inner = {}
  complex_dict_inner = {}
  compound_dict_inner = {}

  for sents in docu.sents:    
    for s in exceptions:
      res = re.search(r"\b" + re.escape(s) + r"\b", sents.text.lower())
      if res:
        break
    
    for s in s_conj:
      res_sj = re.search(r"\b" + re.escape(s) + r"\b", sents.text.lower())
      if res_sj:
        break

    if testSVOs(sents.text) == 1 and not res_sj:#any(tok in tok_list for tok in s_conj):                    # Simple sentence as only 1 subject verb pair and no subord conj
      simple_dict_inner["startIndex"] = sents.start_char
      simple_dict_inner["endIndex"]   = sents.end_char
      simple_dict_inner["text"]       = sents.text 
      simple_list.append(simple_dict_inner.copy()) 
    elif res_sj and not res:#any(tok in tok_list for tok in s_conj) and not res:                            # Complex sentence as more than 1 subject verb pair and subord conjunction.
      complex_dict_inner["startIndex"] = sents.start_char
      complex_dict_inner["endIndex"]   = sents.end_char
      complex_dict_inner["text"]       = sents.text 
      complex_list.append(complex_dict_inner.copy())     
    else:
      compound_dict_inner["startIndex"] = sents.start_char
      compound_dict_inner["endIndex"]   = sents.end_char
      compound_dict_inner["text"]       = sents.text 
      compound_list.append(compound_dict_inner.copy()) 

  ret_dict['Complex-compound'] = dict(color = "#FFA500", description = "Complex compound", highlights= complex_comp_list)
  ret_dict['Complex']          = dict(color = "#90EE90", description = "Complex", highlights = complex_list)
  ret_dict['Compound']         = dict(color = "#FFA07A", description = "Compound", highlights = compound_list)
  ret_dict['Simple']           = dict(color = "#B0C4DE", description = "Simple", highlights = simple_list)

  return (ret_dict)
# Sentence type functionality end


## Monotony functionality
def get_sent_type_tags(essay_text):
  c_conj = ['for','and','but','or','yet','so'] # fanboys coord conj
  pos_list=[]
  tok_list=[]
  complex_comp_list=[]
  simple_list=[]
  compound_list=[]
  sent_offset_list = []
  sent_list=[]
  dep_list = []
  ret_dict = {}
  sent_len = []
  docu = nlp(essay_text)
  doc_sents = [sent for sent in docu.sents]
  len_tot_sents = len(doc_sents)
  for i,sents in enumerate(docu.sents):
    dep_list = []
    pos_list=[]
    tok_list=[]
    for token in sents:# Tokenize the sentence into words/tokens
        dep_list.append(token.dep_)
        pos_list.append(token.pos_)
        tok_list.append(token.lower_)

    # print(sents)
    # print(dep_list)
    print()
    #if ('advcl') in dep_list or ('ccomp') in dep_list or ('xcomp') in dep_list or ('csubj') in dep_list or ('acl') in dep_list or ('csubjpass') in dep_list or ('relcl') in dep_list:
    if any(dep in dep_list for dep in ['advcl','xcomp','csubj','acl','csubjpass','relcl']) or all (dep in dep_list for dep in ['ccomp','prep']):
      sent_list.append('CX')#'Complex-compound sentence')
      complex_comp_list.append([sents])
      sent_offset_list.append(sents.start_char)
      sent_offset_list.append(sents.end_char)
    elif any(dep in pos_list for dep in ['CCONJ', 'SCONJ','CONJ']) or any(tok in tok_list for tok in c_conj):
      sent_list.append('CO')#'Compound sentence')
      compound_list.append([sents])
      sent_offset_list.append(sents.start_char)
      sent_offset_list.append(sents.end_char)
    else:
      sent_list.append('SI')#'Simple sentence')
      simple_list.append([sents])
      sent_offset_list.append(sents.start_char)
      sent_offset_list.append(sents.end_char)
    
    sent_len.append(len(sents))

  ret_dict['Complex']          = (complex_comp_list)
  ret_dict['Compound']         = (compound_list)
  ret_dict['Simple']           = (simple_list)
  ret_dict['Total']            = len_tot_sents

  return (''.join(sent_list),sent_len,sent_offset_list)


def check_token_repetition(essay_text,batch_size,increment_size,numgrams,threshold):
  mylist = []
  offset_list = []
  doc = nlp(essay_text)
  start_index = 0

  # if document length is smaller than the default window size
  if batch_size > len(doc):
    #print((str(doc[start_index:batch_size]).lower().split()))
    grams = Counter(ngrams(str(doc[start_index:batch_size]).lower().split(), numgrams))
    #print(sorted(grams, key=None, reverse=False))
    #sorted_grams = sorted(grams.items(), key=lambda pair: pair[1], reverse=True)
    #print(type(grams.most_common()))
    #pprint.pprint(grams)
    for segment,occurence in grams.items():
      #print(segment,occurence)
      if occurence >= threshold:
        if [' '.join(segment),occurence] not in mylist:
          mylist.append([' '.join(segment),occurence])
    start_index += 1
    batch_size +=  1
    #print(mylist)
    # print()
    # print(essay_text.lower())
    if mylist:
      #for val in mylist:
      offset_list.append([(match.start(),match.end()+1) for match in re.finditer(str(mylist[0][0]), essay_text.lower())])
  else: # if document length is bigger than the default window size
    while batch_size <= len(doc):  
      grams = Counter(ngrams(str(doc[start_index:batch_size]).lower().split(), numgrams))
      for segment,occurence in grams.items():
        #print(segment,occurence)
        if occurence >= threshold:
          if [' '.join(segment),occurence] not in mylist:
            mylist.append([' '.join(segment),occurence])
      start_index += increment_size
      batch_size +=  increment_size
    if mylist:
      #print(mylist)
      #for val in mylist:
      offset_list.append([(match.start(),match.end()+1) for match in re.finditer(str(mylist[0][0]), essay_text.lower())])
      #print(mylist[0][0],numgrams)


  #print(mylist)
  return offset_list

@router.post("/check_monotony/")
async def check_monotony(inputEssay:Essay):
  sent_len=[]
  sent_offset=[]
  char_offset_list = []
 # output_pattern,sent_len,sent_offset  = get_sent_type_tags(input_essay)
  window_size = 200 # default value
  increment_size = 5 # default value
  ret_dict = {}
  
  # if np.std(sent_len) < 1.0:  # Standard deviation is less than 1, indicates that sentences are of very similar length.
  #   ret_dict['Monotonous'] = 'YES'
  #   ret_dict['Description']  = "Essay is monotonous because individual sentences are of very similar length."
  #   ret_dict['Offset'] = [sent_offset[0],sent_offset[-1]] 
  #   if sent_len: 
  #     print(sent_len)
  #     print(sent_offset)
  #     print(output_pattern)
  #   return ret_dict

  # pattern = 'SISISISISI' # pattern to match consecutive 5 simple sentences.

  # if re.search(f'({pattern})+', output_pattern):
  #   ret_dict['Monotonous'] = 'YES'
  #   ret_dict['Description']  = "Essay is monotonous because of repetition of 5 or more Simple sentences consecutively." + output_pattern 
    
  #   for m in re.finditer(pattern, output_pattern):
  #     st = int(m.start(0))        # Finding matching pattern starting indices
  #     en = int(m.end(0))-1         # Finding matching pattern ending indices
    
  #   ret_dict['Offset'] = [sent_offset[st],sent_offset[en]]
  #   if sent_len: 
  #     print(sent_offset[st],sent_offset[en])
  #     print(sent_len)
  #     print(sent_offset)
  #     print(output_pattern)    

  #   return ret_dict

  # pattern = 'COCOCO' #CXCXCX   # pattern to match consecutive 3 complex sentences.

  # if re.search(f'({pattern})+', output_pattern):
  #   print([(m.start(0), m.end(0)) for m in re.finditer(pattern, output_pattern)])
  #   ret_dict['Monotonous'] = 'YES'
  #   ret_dict['Description']  = "Essay is monotonous because of repetition of 3 or more Complex sentences consecutively." + output_pattern

  #   for m in re.finditer(pattern, output_pattern):
  #     st = int(m.start(0))        # Finding matching pattern starting indices
  #     en = int(m.end(0))-1         # Finding matching pattern ending indices
    
  #   ret_dict['Offset'] = [sent_offset[st],sent_offset[en]]    
  #   return ret_dict

  mylist = re.findall(r'[^!?.,-;]+',inputEssay.EssayText) # To remove punctuations
  input_essay_wo_p = ' '.join(mylist)

  numgrams=5
  threshold=2

  char_offset_list = check_token_repetition(input_essay_wo_p,window_size,increment_size,numgrams,threshold)

  if char_offset_list:
    ret_dict['Monotonous'] = 'Yes'
    ret_dict['Description']  = "Essay is monotonous because of repetition of " + str(numgrams) + " tokens " + str(threshold) + " or more number of times!"
    ret_dict['Offset'] = char_offset_list

    return ret_dict

  numgrams=4
  threshold=2  

  char_offset_list = check_token_repetition(input_essay_wo_p,window_size,increment_size,numgrams,threshold)
  if char_offset_list:
    ret_dict['Monotonous'] = 'YES'
    ret_dict['Description']  = "Essay is monotonous because of repetition of " + str(numgrams) + " tokens " + str(threshold) + " or more number of times!"  
    ret_dict['Offset'] = char_offset_list

    return ret_dict

  numgrams=3
  threshold=3  

  char_offset_list = check_token_repetition(input_essay_wo_p,window_size,increment_size,numgrams,threshold)
  if char_offset_list:
    ret_dict['Monotonous'] = 'YES'
    ret_dict['Description']  = "Essay is monotonous because of repetition of " + str(numgrams) + " tokens " + str(threshold) + " or more number of times!"
    ret_dict['Offset'] = char_offset_list

    return ret_dict

  numgrams=2
  threshold=4

  char_offset_list = check_token_repetition(input_essay_wo_p,window_size,increment_size,numgrams,threshold)
  if char_offset_list:
    ret_dict['Monotonous'] = 'YES'
    ret_dict['Description']  = "Essay is monotonous because of repetition of " + str(numgrams) + " tokens " + str(threshold) + " or more number of times!"
    ret_dict['Offset'] = char_offset_list
    return ret_dict
    
  ret_dict['Monotonous'] = 'No'
  ret_dict['Description']  = "Essay is not monotonous."
  ret_dict['Offset'] = []  
  return ret_dict

  # N-GRAM PREDICTIONS
#from .essayGraderDL import score_main
from .essayGrader import score_main # with 16 features
from .essayGraderV2 import score_main_v2 # with 27 features


@router.post("/predict_grade")
async def grade_predictions(inputEssay:Essay):
    return score_main(inputEssay.EssayText)

@router.post("/predict_grade_v2")
async def grade_predictions_v2(inputEssay:Essay):
    return score_main_v2(inputEssay.EssayText)



@router.post("/sentence_shortening/")
async def sentence_shortening(inputEssay:Essay):
  df = pd.read_excel("proof_reading/shorter_text.xlsx")
  df_new =df.values.tolist()
  
  #to get rid of 'nan' values
  l1 =[]
  l2=[]
  for i in df_new:
    for j in i:
      if str(j)!='nan':
        l2.append(j)
    l1.append(l2)     # formation of vocabulary out of xlsx file complete.
    l2=[]

    # following code does traversing through the xlsx file to find matches.

  d1 ={}
  blist =[]
  short_text_dict = {}
  short_text_list = []
  for i in l1:                  # Forming a list or vocabulary out of a spreadsheet file.
    f1=str(i[0]).split(" ")[:1]
    f2=str(i[0]).split(" ")[1:]
    if f1 not in blist:
      #print("".join(f1))
      d1[" ".join(f1).lower()] ={" ".join(f2): i[1:]}
    else:
      d1[" ".join(f1).lower()] [" ".join(f2)] = i[1:]
    blist.append(f1)

  #pprint.pprint(d1)

  #making some changes to the text for better matching
  #text = text.lower()
  text = inputEssay.EssayText.replace('.', ' ')
  text = inputEssay.EssayText.replace(',', ' ')
  text = inputEssay.EssayText.replace('?', " ")
  text = inputEssay.EssayText.replace('!', " ")

  w_list = text.lower().split()                       # tokenizing the text using split function.
  w_list2 = text.split()                              # tokenizing the text and not lowering for printing.

  #searching for the match
  for i in range(0, len(w_list)):
    if w_list[i] in d1:                                               # if there is an initial match to the vocab!
      index = i
      for j in d1[w_list[i]]:
        if(j == " ".join(w_list[index+1:index+1+len(j.split(" "))])): # if the further portion of the initial matched text matches as well!
          #short_text_dict['Offset'] = [(match.start(),match.end()) for match in re.finditer(str(w_list[i] + ' ' + j), text.lower())]
          short_text_dict['Matched text'] = str(w_list2[i] + ' ' + j)
          short_text_dict['Replacement text'] = d1[w_list[i]][j]

          if short_text_dict not in short_text_list:
            short_text_list.append(short_text_dict.copy()) 
          
  ret_dict = dict(color = "#FFA500", description = "Text shortening",short_text = short_text_list)

  # Looping over the dictionary creared inorder to format the output inform of 
  # Input- They are able to do it
  # Output- They <ss edit="can">are able to</ss> do it
  textval = inputEssay.EssayText

  for k,v in ret_dict.items():
    if k == 'short_text':
      for value in v:
        for k2,v2 in value.items():
          if k2 == 'Matched text':
            for k3,v3 in value.items():
              if k3 == 'Replacement text':
                #replace_string = '<ss edit=' + '"' + v2 + '"' + '>' + str(v3) + '<ss>'
                replace_string = '<ss edit=' + '\'' + v2 + '\'' + '>' + str(v3) + '</ss>'
                textval = textval.replace(v2,replace_string)
  
  return textval

@router.post("/cliche_detection/")
async def cliche_detection(inputEssay:Essay):
  df = pd.read_excel("proof_reading/spotting_cliches.xlsx")
  df_new =df.values.tolist()
  
  #to get rid of 'nan' values
  l1 =[]
  l2=[]
  for i in df_new:
    for j in i:
      if str(j)!='nan':
        l2.append(j)
    l1.append(l2)     # formation of vocabulary out of xlsx file complete.
    l2=[]

    # following code does traversing through the xlsx file to find matches.

  d1 ={}
  blist =[]
  cliche_phrase_dict = {}
  cliche_phrase_list = []
  for i in l1:                  # Forming a list or vocabulary out of a spreadsheet file.
    f1=str(i[0]).split(" ")[:1]
    f2=str(i[0]).split(" ")[1:]
    if f1 not in blist:
      #print("".join(f1))
      d1[" ".join(f1).lower()] ={" ".join(f2): i[1:]}
    else:
      d1[" ".join(f1).lower()] [" ".join(f2)] = i[1:]
    blist.append(f1)

  #pprint.pprint(d1)

  #making some changes to the text for better matching
  #text = text.lower()
  #print(inputEssay.EssayText)
  text = inputEssay.EssayText.replace('.', ' ')
  text = text.replace(',', ' ')
  text = text.replace('?', " ")
  text = text.replace('!', " ")
  #print(text)

  w_list = text.lower().split()                       # tokenizing the text using split function.
  w_list2 = text.split()                              # tokenizing the text and not lowering for printing.

  #searching for the match
  for i in range(0, len(w_list)):
    if w_list[i] in d1:                                               # if there is an initial match to the vocab!
      index = i
      for j in d1[w_list[i]]:
        if(j == " ".join(w_list[index+1:index+1+len(j.split(" "))])): # if the further portion of the initial matched text matches as well!
          #print(" ".join(w_list[index+1:index+1+len(j.split(" "))]))
          #print(j)
          cliche_phrase_dict['Offset'] = [(match.start(),match.end()) for match in re.finditer(str(w_list[i] + ' ' + j), text.lower())]
          cliche_phrase_dict['Cliche_phrase'] = str(w_list2[i] + ' ' + j)

          if cliche_phrase_dict not in cliche_phrase_list:
            cliche_phrase_list.append(cliche_phrase_dict.copy()) 
          
  return dict(color = "#FFA500", description = "Cliche detection",cliche_list = cliche_phrase_list)

import text2emotion as te
import pickle
from fastapi import  HTTPException

with open(r"models/informal_word_list.pickle", "rb") as input_file:
      word_lists = pickle.load(input_file)
      
@router.post("/emotion_detection")
async def tone_detection(InputEssay:Essay):
        text=InputEssay.EssayText
        essay=[w.lower() for w in text.split()]
        if len(essay)>100:
            essay=set(essay)
            emotions=te.get_emotion(text)
            formality=(len(essay.intersection(word_lists))/len(essay))
            # score=(formality*(5))
            emotions["infomality_score"]=formality
            return emotions
        else:
            raise HTTPException(status_code=400, detail=f"Essay word count must be greater than 120")

# tone_detection(txt)